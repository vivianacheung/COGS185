{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Bonus_Task2_Skeleton.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wRUD3Hsz5iXt"
      },
      "source": [
        "!pip3 install pytorch-transformers==1.0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0gmA2RyA59nx"
      },
      "source": [
        "!pip3 install boto3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lbt-buwT7mb4"
      },
      "source": [
        "# NOTE: This is specific to Google Colab to change the working directory. Use this to point to the path that contains Sarcasm_Headlines_Dataset.json file.\n",
        "from google.colab import drive \n",
        "drive.mount('/content/drive')\n",
        "%cd /content/drive/My\\ Drive/Colab\\ Notebooks"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7K-klZls8CtG"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPGbF0Rz5jXE"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from pytorch_transformers import BertModel, BertTokenizer, BertConfig, WarmupLinearSchedule\n",
        "import re\n",
        "import pandas as pd\n",
        "import json\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "def seed_everything(seed = 42):\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "\n",
        "# For reproducible results\n",
        "seed_everything()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PK8fs2QG5nHh"
      },
      "source": [
        "# Constants\n",
        "\n",
        "SEP_TOKEN = '[SEP]'\n",
        "CLS_TOKEN = '[CLS]'\n",
        "TRAIN_FILE_PATH = 'Sarcasm_Headlines_Dataset.json'\n",
        "MAX_SEQ_LENGTH = 512\n",
        "BATCH_SIZE = 4\n",
        "NUM_EPOCHS = 6\n",
        "GRADIENT_ACCUMULATION_STEPS = 8\n",
        "WARMUP_STEPS = 3\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0n04U6d6tQH"
      },
      "source": [
        "class BertClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super(BertClassifier, self).__init__()\n",
        "        # Binary classification problem (num_labels = 2)\n",
        "        self.num_labels = config.num_labels\n",
        "        # Pre-trained BERT model\n",
        "        self.bert = BertModel(config)\n",
        "        # Dropout to avoid overfitting\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        # A single layer classifier added on top of BERT to fine tune for binary classification\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "        # Weight initialization\n",
        "        torch.nn.init.xavier_normal_(self.classifier.weight)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None,\n",
        "                position_ids=None, head_mask=None):\n",
        "        # Forward pass through pre-trained BERT\n",
        "        outputs = self.bert(input_ids, position_ids=position_ids, token_type_ids=token_type_ids,\n",
        "                            attention_mask=attention_mask, head_mask=head_mask)\n",
        "        \n",
        "        # Last layer output (Total 12 layers)\n",
        "        pooled_output = outputs[-1]\n",
        "        pooled_output = pooled_output.detach()   # Stop gradient to prevent bert encoder from being fine-tuned\n",
        "\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        return self.classifier(pooled_output)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6SpRUed684i"
      },
      "source": [
        "class SequenceDataset(Dataset):\n",
        "    def __init__(self, dataset_file_path, tokenizer, regex_transformations={}):\n",
        "        # Read JSON file and assign to headlines variable (list of strings)\n",
        "        df = pd.read_json(dataset_file_path, lines=True)\n",
        "        print (df)\n",
        "        df = df.drop(['article_link'], axis=1)\n",
        "        self.headlines = df.values\n",
        "        # Regex Transformations can be used for data cleansing.\n",
        "        # e.g. replace \n",
        "        #   '\\n' -> ' ', \n",
        "        #   'wasn't -> was not\n",
        "        self.regex_transformations = regex_transformations\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.headlines)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        headline, is_sarcastic = self.headlines[index]\n",
        "        for regex, value_to_replace_with in self.regex_transformations.items():\n",
        "            headline = re.sub(regex, value_to_replace_with, headline)\n",
        "\n",
        "        # Convert input string into tokens with the special BERT Tokenizer which can handle out-of-vocabulary words using subgrams\n",
        "        # e.g. headline = Here is the sentence I want embeddings for.\n",
        "        #      tokens = [here, is, the, sentence, i, want, em, ##bed, ##ding, ##s, for, .]\n",
        "        tokens = self.tokenizer.tokenize(headline)\n",
        "\n",
        "        # Add [CLS] at the beginning and [SEP] at the end of the tokens list for classification problems\n",
        "        tokens = [CLS_TOKEN] + tokens + [SEP_TOKEN]\n",
        "        # Convert tokens to respective IDs from the vocabulary\n",
        "        input_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n",
        "\n",
        "        # Segment ID for a single sequence in case of classification is 0. \n",
        "        segment_ids = [0] * len(input_ids)\n",
        "\n",
        "        # Input mask where each valid token has mask = 1 and padding has mask = 0\n",
        "        input_mask = [1] * len(input_ids)\n",
        "\n",
        "        # padding_length is calculated to reach max_seq_length\n",
        "        padding_length = MAX_SEQ_LENGTH - len(input_ids)\n",
        "        input_ids = input_ids + [0] * padding_length\n",
        "        input_mask = input_mask + [0] * padding_length\n",
        "        segment_ids = segment_ids + [0] * padding_length\n",
        "\n",
        "        assert len(input_ids) == MAX_SEQ_LENGTH\n",
        "        assert len(input_mask) == MAX_SEQ_LENGTH\n",
        "        assert len(segment_ids) == MAX_SEQ_LENGTH\n",
        "\n",
        "        return torch.tensor(input_ids, dtype=torch.long, device=DEVICE), \\\n",
        "               torch.tensor(segment_ids, dtype=torch.long, device=DEVICE), \\\n",
        "               torch.tensor(input_mask, device=DEVICE), \\\n",
        "               torch.tensor(is_sarcastic, dtype=torch.long, device=DEVICE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRbQdysm6--a"
      },
      "source": [
        "# Load BERT default config object and make necessary changes as per requirement\n",
        "config = BertConfig(hidden_size=768,\n",
        "                    num_hidden_layers=12,\n",
        "                    num_attention_heads=12,\n",
        "                    intermediate_size=3072,\n",
        "                    num_labels=2)\n",
        "\n",
        "# Create our custom BERTClassifier model object\n",
        "model = BertClassifier(config)\n",
        "model.to(DEVICE)\n",
        "\n",
        "# Initialize BERT tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qRWOgJQ-7Nq"
      },
      "source": [
        "#train_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1z1guolT7B4a"
      },
      "source": [
        "# Load Train dataset and split it into Train and Validation dataset\n",
        "train_dataset = SequenceDataset(TRAIN_FILE_PATH, tokenizer)\n",
        "\n",
        "validation_split = 0.4\n",
        "\n",
        "dataset_size = len(train_dataset)\n",
        "indices = list(range(dataset_size))\n",
        "split = int(np.floor(validation_split * dataset_size))\n",
        "shuffle_dataset = True\n",
        "\n",
        "if shuffle_dataset :\n",
        "    np.random.shuffle(indices)\n",
        "train_indices, val_indices = indices[split:], indices[:split]\n",
        "\n",
        "train_sampler = SubsetRandomSampler(train_indices)\n",
        "validation_sampler = SubsetRandomSampler(val_indices)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=train_sampler)\n",
        "val_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, sampler=validation_sampler)\n",
        "\n",
        "print ('Training Set Size {}, Validation Set Size {}'.format(len(train_indices), len(val_indices)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wCIBOk7y7J9k"
      },
      "source": [
        "# Loss Function\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Adam Optimizer with very small learning rate given to BERT\n",
        "optimizer = torch.optim.Adam([\n",
        "                #{'params': model.bert.parameters(), 'lr' : 1e-5},\n",
        "                {'params': model.classifier.parameters(), 'lr': ###### YOUR CODE HERE ######}\n",
        "            ])\n",
        "\n",
        "# Learning rate scheduler\n",
        "scheduler = WarmupLinearSchedule(optimizer, warmup_steps=WARMUP_STEPS, t_total=len(train_loader) // GRADIENT_ACCUMULATION_STEPS * NUM_EPOCHS)\n",
        "\n",
        "model.zero_grad()\n",
        "epoch_iterator = trange(int(NUM_EPOCHS), desc=\"Epoch\")\n",
        "training_acc_list, validation_acc_list = [], []\n",
        "\n",
        "for epoch in epoch_iterator:\n",
        "    epoch_loss = 0.0\n",
        "    train_correct_total = 0\n",
        "\n",
        "    # Training Loop\n",
        "    train_iterator = tqdm(train_loader, desc=\"Train Iteration\")\n",
        "    for step, batch in enumerate(train_iterator):\n",
        "        model.train(True)\n",
        "        # Here each element of batch list refers to one of [input_ids, segment_ids, attention_mask, labels]\n",
        "        inputs = {\n",
        "            'input_ids': batch[0].to(DEVICE),\n",
        "            'token_type_ids': batch[1].to(DEVICE),\n",
        "            'attention_mask': batch[2].to(DEVICE)\n",
        "        }\n",
        "\n",
        "        labels = batch[3].to(DEVICE)\n",
        "        logits = model(**inputs)\n",
        "\n",
        "        loss = criterion(logits, labels) / GRADIENT_ACCUMULATION_STEPS\n",
        "        loss.backward()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "        if (step + 1) % GRADIENT_ACCUMULATION_STEPS == 0:\n",
        "            scheduler.step()\n",
        "            optimizer.step()\n",
        "            model.zero_grad()\n",
        "\n",
        "        _, predicted = torch.max(logits.data, 1)\n",
        "        correct_reviews_in_batch = (predicted == labels).sum().item()\n",
        "        train_correct_total += correct_reviews_in_batch\n",
        "    \n",
        "    print('Epoch {} - Loss {:.2f}'.format(epoch + 1, epoch_loss / len(train_indices)))\n",
        "\n",
        "    # Validation Loop\n",
        "    with torch.no_grad():\n",
        "        val_correct_total = 0\n",
        "        model.train(False)\n",
        "        val_iterator = tqdm(val_loader, desc=\"Validation Iteration\")\n",
        "        for step, batch in enumerate(val_iterator):\n",
        "            inputs = {\n",
        "                'input_ids': batch[0].to(DEVICE),\n",
        "                'token_type_ids': batch[1].to(DEVICE),\n",
        "                'attention_mask': batch[2].to(DEVICE)\n",
        "            }\n",
        "\n",
        "            labels = batch[3].to(DEVICE)\n",
        "            logits = model(**inputs)\n",
        "\n",
        "            _, predicted = torch.max(logits.data, 1)\n",
        "            correct_reviews_in_batch = (predicted == labels).sum().item()\n",
        "            val_correct_total += correct_reviews_in_batch\n",
        "\n",
        "        training_acc_list.append(train_correct_total * 100 / len(train_indices))\n",
        "        validation_acc_list.append(val_correct_total * 100 / len(val_indices))\n",
        "        print('Training Accuracy {:.4f} - Validation Accurracy {:.4f}'.format(train_correct_total * 100 / len(train_indices), val_correct_total * 100 / len(val_indices)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vGWESZw58LB6"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs_list = list(range(1, NUM_EPOCHS + 1))\n",
        "plt.plot(epochs_list, training_acc_list, color='g')\n",
        "plt.plot(epochs_list, validation_acc_list, color='orange')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Training vs Validation Accuracy')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ftn2OdW-8Qqz"
      },
      "source": [
        "train_correct_total * 100 / len(train_indices)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzhTOXtI-fWh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}