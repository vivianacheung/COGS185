{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings; warnings.simplefilter('ignore')\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "oe = preprocessing.OneHotEncoder(sparse=False)\n",
    "import dlib\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "import timeit\n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "import pickle\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 3: Structured SVM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Windows Length\n",
    "L = 2\n",
    "# Number of examples\n",
    "N = 5000\n",
    "# Length of a feature\n",
    "d = 128\n",
    "# The hyper-parameter for icm search\n",
    "Niter = 2               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2i(a):\n",
    "        return int(ord(a)-ord('a'))\n",
    "def i2l(i):\n",
    "    if i >= 0:\n",
    "        return chr(i+ord('a'))\n",
    "    else:\n",
    "        return '_'\n",
    "def iors(s):\n",
    "    try:\n",
    "        return int(s)\n",
    "    except ValueError: # if it is a string, return a string\n",
    "        return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the entire dataset into lists or list of lists\n",
    "def read_OCR(filename, n_features):\n",
    "    F = open(filename)\n",
    "    dataset = {}\n",
    "    dataset['ids'] = []#np.zeros(n_examples, dtype=int)\n",
    "    dataset['labels'] = []#np.zeros(n_examples,dtype=int)\n",
    "    dataset['labelDic'] = {} # To profile the distribution of labels\n",
    "    dataset['next_ids'] = []#np.zeros(n_examples,dtype=int)\n",
    "    dataset['word_ids'] = []#np.zeros(n_examples,dtype=int)\n",
    "    dataset['positions'] = []#np.zeros(n_examples,dtype=int)\n",
    "    dataset['folds'] = []#np.zeros(n_examples,dtype=int)\n",
    "    dataset['features'] = []#np.zeros([n_examples,n_features])\n",
    "    \n",
    "    for str_line in F.readlines():\n",
    "        #line0 = map(iors, filter(None, re.split('\\t', str_line.strip())))\n",
    "        ## ATTENTION: If you are using Python3, use the following line instead\n",
    "        line0 = list(map(iors, filter(None, re.split('\\t', str_line.strip()))))\n",
    "\n",
    "\n",
    "        dataset['ids'].append(int(line0.pop(0)))\n",
    "        dataset['labels'].append(l2i(line0.pop(0))) # The label is converted into integer('a'=>0, 'z'=>25)\n",
    "        if dataset['labels'][-1] in dataset['labelDic']:\n",
    "            dataset['labelDic'][dataset['labels'][-1]] += 1\n",
    "        else:\n",
    "            dataset['labelDic'][dataset['labels'][-1]] = 1\n",
    "            \n",
    "        dataset['next_ids'].append(int(line0.pop(0)))\n",
    "        dataset['word_ids'].append(int(line0.pop(0)))\n",
    "        dataset['positions'].append(int(line0.pop(0)))\n",
    "        dataset['folds'].append(int(line0.pop(0)))\n",
    "        if len(line0) != 128:  # Sanity check of the length\n",
    "            print (len(line0))\n",
    "        dataset['features'].append(line0)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset1 = read_OCR('OCRdataset/letter.data', d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Understand the profile of OCR raw data\n",
    "print (\"max of labels=\", max(dataset1['labels']), \" min of labels=\", min(dataset1['labels']), 'num of labels=', len(dataset1['labelDic']))\n",
    "print (\"labelDic.keys()=\", map(i2l, dataset1['labelDic'].keys()))\n",
    "print (\"Total number of lines=\", len(dataset1['ids']))\n",
    "print (\"The shape of features:\", np.array(dataset1['features']).shape)\n",
    "\n",
    "print (\"The first 10 ids:\",dataset1['ids'][:10])\n",
    "print (\"ids[0]=\",dataset1['ids'][0])\n",
    "print (\"labels[0]=\", dataset1['labels'][0])\n",
    "print (\"The 1st letter is \", i2l(dataset1['labels'][0]))\n",
    "print (\"next_ids[0]=\",dataset1['next_ids'][0])\n",
    "# Show the matrix into an image\n",
    "def showFeatures(features, num):\n",
    "    plt.figure(figsize=(num, 6))\n",
    "    \n",
    "    for i in range(num):\n",
    "        npfeature = np.array(features[i])\n",
    "        plt.subplot(1,num,i+1)\n",
    "        imshow(npfeature.reshape(16,8), cmap='gray')\n",
    "        plt.title(i)\n",
    "\n",
    "showFeatures(dataset1['features'],20)\n",
    "\n",
    "# f1 = np.array(dataset1['features'])\n",
    "# f1 = np.hstack([np.ones((f1.shape[0],1)), f1])\n",
    "# print f1.shape\n",
    "# dataset1['features'] = f1.tolist()\n",
    "# d +=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Options for Chopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenating and structurizing\n",
    "### version one:\n",
    "####  (1) find 5000 words, split them into 4000 for training, and 1000 for testing. \n",
    "####  (2) chop first 2 characters from each word\n",
    "####  (3) then, you will have 4000 two-words pairs for training, and 1000 for testing. \n",
    "####  (4) construct new structures data based on this pairs. \n",
    " \n",
    "  e.g.\n",
    "  \n",
    "  apple => ap\n",
    "  \n",
    "  banana => ba\n",
    "  \n",
    " \n",
    "### version two:\n",
    "####  (1) continue reading words from data until you have 4001 characters for training, and 1001 for testing\n",
    "####  (2) concatenate characters together to form structure features. \n",
    "####  (3) you will have the same amount structured data. \n",
    "  apple => ap, pp, pl, le\n",
    " \n",
    " banana => ba, an, na, an, na\n",
    "  \n",
    "### version three\n",
    "####  (1) similar to version two, but add a dummy letter '_' between words.\n",
    "  apple_banana => ap, pp, pl, le, e_, _b, ba, an, na, an, na\n",
    " \n",
    "### version four\n",
    "#### (1) similar, but window stride is 2 also, and pad a dummy letter '_', which will be represented as all 0 in the image. \n",
    "  apple => ap, pl, e_\n",
    "  \n",
    "  banana => ba, na,  na"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Extract the first L letters in a word\n",
    "# You are welcome to try other options\n",
    "\n",
    "def structurize1(dataset, N, L):\n",
    "    d_features = len(dataset['features'][0])\n",
    "    y = dataset['labels']\n",
    "    X = dataset['features']\n",
    "    next_id = dataset['next_ids']\n",
    "\n",
    "    labels = np.zeros((N, L))\n",
    "    features = np.zeros((N, L*d_features))\n",
    "    \n",
    "    # Extract only one structured example\n",
    "    def extract(iN, loc):\n",
    "        labels[iN] = y[loc:loc+L]\n",
    "        features[iN] = np.array(X[loc:loc+L]).ravel().tolist()\n",
    "        iN += 1\n",
    "        return iN\n",
    "    \n",
    "    iN = 0\n",
    "    iN = extract(iN, 0)\n",
    "    \n",
    "    for key, value in enumerate(y):\n",
    "        if next_id[key] == -1:\n",
    "            iN = extract(iN, key+1)\n",
    "            \n",
    "            if iN == N:\n",
    "                break\n",
    "    \n",
    "    c = list(zip(labels, features))\n",
    "    random.shuffle(c)\n",
    "    labels, features = zip(*c)\n",
    "    \n",
    "    return np.array(labels), np.array(features)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1, features1 = structurize1(dataset1, N, L)\n",
    "\n",
    "print (np.all(labels1[labels1==labels1[0]]))\n",
    "print (labels1[:15].T)\n",
    "showFeatures(features1[:,0:128],15)\n",
    "showFeatures(features1[:,128:256],15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use dlib to classify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('N=',N,'L=',L,'d=',d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ThreeClassProblem:\n",
    "    C = 1\n",
    "\n",
    "    def __init__(self, samples, labels, L, K, d):\n",
    "        self.L = L\n",
    "        self.K = K\n",
    "        self.d = d\n",
    "        self.num_samples = len(samples)\n",
    "        self.num_dimensions = (L*K*d+1) + (L-1)\n",
    "        self.samples = samples\n",
    "        self.labels = labels\n",
    "        self.loss_for_loop = True\n",
    "        \n",
    "    def make_psi(self, x, label):\n",
    "       \n",
    "        psi = dlib.vector()\n",
    "        psi.resize(self.num_dimensions)\n",
    "        psi[0] = x[0] # The bias\n",
    "        \n",
    "        ########### YOUR CODE HERE ###########\n",
    "        \n",
    "        return psi\n",
    "\n",
    "    def get_truth_joint_feature_vector(self, idx):\n",
    "        return self.make_psi(self.samples[idx], self.labels[idx])\n",
    "    \n",
    "    def separation_oracle(self, idx, current_solution):\n",
    "        samp = self.samples[idx]\n",
    "        psi = [0]*self.num_dimensions\n",
    "        max1 = -1e10\n",
    "        max_scoring_label = [0]*L # Initialize max_scoring_label for icm search\n",
    "        for k in range(Niter):\n",
    "            for iL in range(self.L):   # Iterate over the window length\n",
    "                for i in range(self.K):# Change different label for the search of a structured label\n",
    "                    tmp_label = ########### YOUR CODE HERE ########### # New a list to avoid modifying the max_scoring_label\n",
    "                    tmp_label[iL] = ########### YOUR CODE HERE ########### # Take turns to modify the structured label from left to right. The guessed structured label.\n",
    "                    tmp_psi = ########### YOUR CODE HERE ########### # Make a new Psi for the guessed structured label\n",
    "                    score1 = dlib.dot(current_solution, tmp_psi)\n",
    "                    \n",
    "                    loss1 = 0.0\n",
    "                    if self.loss_for_loop:\n",
    "                        for j in range(self.L):\n",
    "                            if self.labels[idx][j] != tmp_label[j]:\n",
    "                                loss1 += 1.0\n",
    "                    else:\n",
    "                        if self.labels[idx] != tmp_label: # Add the conditional \"1\"\n",
    "                            loss1 += 1.0\n",
    "\n",
    "                    if max1 < score1+loss1: # Search for the maximum and update loss, max_scoring_label, and psi\n",
    "                        max1 = score1 + loss1\n",
    "                        loss = loss1\n",
    "                        max_scoring_label[iL] = i\n",
    "                        psi = tmp_psi\n",
    "\n",
    "        return loss, psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_accuracy(samples, labels, problem, weights, K):\n",
    "    predictions = []\n",
    "    for samp in samples:\n",
    "        prediction = [0]*L # Initialize max_scoring_label for icm search\n",
    "        Niter = 2                 # The hyper-parameter for icm search\n",
    "        max1 = -1e10              # The max value during maximizing our target function\n",
    "        for k in range(Niter):\n",
    "            for iL in range(L):   # Iterate over the window length\n",
    "                for i in range(K):# Change differnet label for the search of a structured label\n",
    "                    tmp_label = ########### YOUR CODE HERE ###########\n",
    "                    tmp_label[iL] = ########### YOUR CODE HERE ###########\n",
    "                    psi1 = ########### YOUR CODE HERE ###########\n",
    "                    score1 = dlib.dot(weights, psi1)\n",
    "\n",
    "                    if max1 < score1:\n",
    "                        max1 = score1\n",
    "                        prediction[iL] = i\n",
    "        predictions.append(prediction)\n",
    "    \n",
    "    \n",
    "    errCnt = 0\n",
    "    for i in range(len(predictions)):\n",
    "        if predictions[i] != labels[i]:\n",
    "            errCnt += 1\n",
    "\n",
    "    return 1.0-float(errCnt)/float(len(predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le1 = preprocessing.LabelEncoder()\n",
    "nplabels1  = le1.fit_transform(labels1.ravel()).reshape(labels1.shape)\n",
    "npsamples1 = np.hstack([np.ones((N,1)), features1]) # Add ones for bias\n",
    "K1 = len(le1.classes_)\n",
    "print ('K1=', K1)\n",
    "\n",
    "tr_labels  = nplabels1[:int(N*0.8)].astype(int).tolist()\n",
    "tr_samples = npsamples1[:int(N*0.8)].astype(int).tolist()\n",
    "te_labels  = nplabels1[int(N*0.8):].astype(int).tolist()\n",
    "te_samples = npsamples1[int(N*0.8):].astype(int).tolist()\n",
    "\n",
    "def profiling(labels):\n",
    "    TrDic = {}\n",
    "    for i in np.array(labels).ravel():\n",
    "        if i not in TrDic:\n",
    "            TrDic[i] = 1\n",
    "        else:\n",
    "            TrDic[i] += 1\n",
    "    return TrDic\n",
    "print (profiling(tr_labels))\n",
    "print (profiling(te_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = ThreeClassProblem(tr_samples, tr_labels, L, K1, d)\n",
    "\n",
    "start_train = timeit.default_timer()\n",
    "weights = dlib.solve_structural_svm_problem(problem)\n",
    "end_train = timeit.default_timer()\n",
    "print (\"Training time elapsed:\", end_train - start_train, \"s\")\n",
    "pickle.dump(weights, open('weights1_1.obj', 'wb'))\n",
    "#pickle.dump(weights, open('weights1_1.obj', 'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_load = pickle.load(open('weights1_1.obj','rb'))\n",
    "#weights_load = pickle.load(open('weights1_1.obj','r'))\n",
    "\n",
    "print (\"Training accuracy=\", cal_accuracy(tr_samples, tr_labels, problem, weights_load, K1))\n",
    "print (\"Test accuracy=\", cal_accuracy(te_samples, te_labels, problem, weights_load, K1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
